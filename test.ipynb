{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,isnan,when,sum,lit\n",
    "from pyspark.sql.functions import corr\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/14 08:29:39 WARN Utils: Your hostname, karyubuntu resolves to a loopback address: 127.0.1.1; using 192.168.0.165 instead (on interface wlp0s20f3)\n",
      "23/03/14 08:29:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/14 08:29:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_object = init_spark()\n",
    "train_identity = spark_object.read.option(\"delimiter\", \",\").csv(\"./data/train_identity.csv\", header='true', inferSchema='true')\n",
    "train_transaction = spark_object.read.option(\"delimiter\", \",\").csv(\"./data/train_transaction.csv\", header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144233\n",
      "590540\n",
      "23/03/13 23:52:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Row(TransactionID=2987004, id_01=0.0, id_02=70787.0, id_03=None, id_04=None, id_05=None, id_06=None, id_07=None, id_08=None, id_09=None, id_10=None, id_11=100.0, id_12='NotFound', id_13=None, id_14=-480.0, id_15='New', id_16='NotFound', id_17=166.0, id_18=None, id_19=542.0, id_20=144.0, id_21=None, id_22=None, id_23=None, id_24=None, id_25=None, id_26=None, id_27=None, id_28='New', id_29='NotFound', id_30='Android 7.0', id_31='samsung browser 6.2', id_32=32.0, id_33='2220x1080', id_34='match_status:2', id_35='T', id_36='F', id_37='T', id_38='T', DeviceType='mobile', DeviceInfo='SAMSUNG SM-G892A Build/NRD90M')\n",
      "Row(TransactionID=2987000, isFraud=0, TransactionDT=86400, TransactionAmt=68.5, ProductCD='W', card1=13926, card2=None, card3=150.0, card4='discover', card5=142.0, card6='credit', addr1=315.0, addr2=87.0, dist1=19.0, dist2=None, P_emaildomain=None, R_emaildomain=None, C1=1.0, C2=1.0, C3=0.0, C4=0.0, C5=0.0, C6=1.0, C7=0.0, C8=0.0, C9=1.0, C10=0.0, C11=2.0, C12=0.0, C13=1.0, C14=1.0, D1=14.0, D2=None, D3=13.0, D4=None, D5=None, D6=None, D7=None, D8=None, D9=None, D10=13.0, D11=13.0, D12=None, D13=None, D14=None, D15=0.0, M1='T', M2='T', M3='T', M4='M2', M5='F', M6='T', M7=None, M8=None, M9=None, V1=1.0, V2=1.0, V3=1.0, V4=1.0, V5=1.0, V6=1.0, V7=1.0, V8=1.0, V9=1.0, V10=0.0, V11=0.0, V12=1.0, V13=1.0, V14=1.0, V15=0.0, V16=0.0, V17=0.0, V18=0.0, V19=1.0, V20=1.0, V21=0.0, V22=0.0, V23=1.0, V24=1.0, V25=1.0, V26=1.0, V27=0.0, V28=0.0, V29=0.0, V30=0.0, V31=0.0, V32=0.0, V33=0.0, V34=0.0, V35=None, V36=None, V37=None, V38=None, V39=None, V40=None, V41=None, V42=None, V43=None, V44=None, V45=None, V46=None, V47=None, V48=None, V49=None, V50=None, V51=None, V52=None, V53=1.0, V54=1.0, V55=1.0, V56=1.0, V57=0.0, V58=0.0, V59=0.0, V60=0.0, V61=1.0, V62=1.0, V63=0.0, V64=0.0, V65=1.0, V66=1.0, V67=1.0, V68=0.0, V69=0.0, V70=0.0, V71=0.0, V72=0.0, V73=0.0, V74=0.0, V75=1.0, V76=1.0, V77=1.0, V78=1.0, V79=0.0, V80=0.0, V81=0.0, V82=0.0, V83=0.0, V84=0.0, V85=0.0, V86=1.0, V87=1.0, V88=1.0, V89=0.0, V90=0.0, V91=0.0, V92=0.0, V93=0.0, V94=0.0, V95=0.0, V96=1.0, V97=0.0, V98=0.0, V99=0.0, V100=0.0, V101=0.0, V102=1.0, V103=0.0, V104=0.0, V105=0.0, V106=0.0, V107=1.0, V108=1.0, V109=1.0, V110=1.0, V111=1.0, V112=1.0, V113=1.0, V114=1.0, V115=1.0, V116=1.0, V117=1.0, V118=1.0, V119=1.0, V120=1.0, V121=1.0, V122=1.0, V123=1.0, V124=1.0, V125=1.0, V126=0.0, V127=117.0, V128=0.0, V129=0.0, V130=0.0, V131=0.0, V132=0.0, V133=117.0, V134=0.0, V135=0.0, V136=0.0, V137=0.0, V138=None, V139=None, V140=None, V141=None, V142=None, V143=None, V144=None, V145=None, V146=None, V147=None, V148=None, V149=None, V150=None, V151=None, V152=None, V153=None, V154=None, V155=None, V156=None, V157=None, V158=None, V159=None, V160=None, V161=None, V162=None, V163=None, V164=None, V165=None, V166=None, V167=None, V168=None, V169=None, V170=None, V171=None, V172=None, V173=None, V174=None, V175=None, V176=None, V177=None, V178=None, V179=None, V180=None, V181=None, V182=None, V183=None, V184=None, V185=None, V186=None, V187=None, V188=None, V189=None, V190=None, V191=None, V192=None, V193=None, V194=None, V195=None, V196=None, V197=None, V198=None, V199=None, V200=None, V201=None, V202=None, V203=None, V204=None, V205=None, V206=None, V207=None, V208=None, V209=None, V210=None, V211=None, V212=None, V213=None, V214=None, V215=None, V216=None, V217=None, V218=None, V219=None, V220=None, V221=None, V222=None, V223=None, V224=None, V225=None, V226=None, V227=None, V228=None, V229=None, V230=None, V231=None, V232=None, V233=None, V234=None, V235=None, V236=None, V237=None, V238=None, V239=None, V240=None, V241=None, V242=None, V243=None, V244=None, V245=None, V246=None, V247=None, V248=None, V249=None, V250=None, V251=None, V252=None, V253=None, V254=None, V255=None, V256=None, V257=None, V258=None, V259=None, V260=None, V261=None, V262=None, V263=None, V264=None, V265=None, V266=None, V267=None, V268=None, V269=None, V270=None, V271=None, V272=None, V273=None, V274=None, V275=None, V276=None, V277=None, V278=None, V279=0.0, V280=0.0, V281=0.0, V282=1.0, V283=1.0, V284=0.0, V285=0.0, V286=0.0, V287=0.0, V288=0.0, V289=0.0, V290=1.0, V291=1.0, V292=1.0, V293=0.0, V294=1.0, V295=0.0, V296=0.0, V297=0.0, V298=0.0, V299=0.0, V300=0.0, V301=0.0, V302=0.0, V303=0.0, V304=0.0, V305=1.0, V306=0.0, V307=117.0, V308=0.0, V309=0.0, V310=0.0, V311=0.0, V312=0.0, V313=0.0, V314=0.0, V315=0.0, V316=0.0, V317=117.0, V318=0.0, V319=0.0, V320=0.0, V321=0.0, V322=None, V323=None, V324=None, V325=None, V326=None, V327=None, V328=None, V329=None, V330=None, V331=None, V332=None, V333=None, V334=None, V335=None, V336=None, V337=None, V338=None, V339=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_identity.count())\n",
    "print(train_transaction.count())\n",
    "print(train_identity.head())\n",
    "print(train_transaction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_transaction.join(train_identity, on=\"TransactionID\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(TransactionID=2987048, isFraud=0, TransactionDT=87317, TransactionAmt=42.294, ProductCD='C', card1=15885, card2=545.0, card3=185.0, card4='visa', card5=138.0, card6='debit', addr1=None, addr2=None, dist1=None, dist2=None, P_emaildomain='outlook.com', R_emaildomain='outlook.com', C1=1.0, C2=1.0, C3=0.0, C4=1.0, C5=0.0, C6=1.0, C7=1.0, C8=1.0, C9=0.0, C10=1.0, C11=1.0, C12=1.0, C13=1.0, C14=1.0, D1=0.0, D2=None, D3=None, D4=0.0, D5=None, D6=0.0, D7=None, D8=None, D9=None, D10=0.0, D11=None, D12=0.0, D13=0.0, D14=0.0, D15=0.0, M1=None, M2=None, M3=None, M4='M2', M5=None, M6=None, M7=None, M8=None, M9=None, V1=None, V2=None, V3=None, V4=None, V5=None, V6=None, V7=None, V8=None, V9=None, V10=None, V11=None, V12=0.0, V13=0.0, V14=1.0, V15=1.0, V16=1.0, V17=1.0, V18=1.0, V19=1.0, V20=1.0, V21=1.0, V22=1.0, V23=1.0, V24=1.0, V25=1.0, V26=1.0, V27=0.0, V28=0.0, V29=0.0, V30=0.0, V31=1.0, V32=1.0, V33=1.0, V34=1.0, V35=0.0, V36=0.0, V37=1.0, V38=1.0, V39=1.0, V40=1.0, V41=1.0, V42=1.0, V43=1.0, V44=1.0, V45=1.0, V46=1.0, V47=1.0, V48=0.0, V49=0.0, V50=1.0, V51=1.0, V52=1.0, V53=0.0, V54=0.0, V55=1.0, V56=1.0, V57=1.0, V58=1.0, V59=1.0, V60=1.0, V61=1.0, V62=1.0, V63=1.0, V64=1.0, V65=1.0, V66=1.0, V67=1.0, V68=0.0, V69=0.0, V70=0.0, V71=1.0, V72=1.0, V73=1.0, V74=1.0, V75=0.0, V76=0.0, V77=1.0, V78=1.0, V79=1.0, V80=1.0, V81=1.0, V82=1.0, V83=1.0, V84=1.0, V85=1.0, V86=1.0, V87=1.0, V88=1.0, V89=0.0, V90=0.0, V91=0.0, V92=1.0, V93=1.0, V94=1.0, V95=0.0, V96=0.0, V97=0.0, V98=0.0, V99=0.0, V100=0.0, V101=0.0, V102=0.0, V103=0.0, V104=0.0, V105=0.0, V106=0.0, V107=1.0, V108=1.0, V109=1.0, V110=1.0, V111=1.0, V112=1.0, V113=1.0, V114=1.0, V115=1.0, V116=1.0, V117=1.0, V118=1.0, V119=1.0, V120=1.0, V121=1.0, V122=1.0, V123=1.0, V124=1.0, V125=1.0, V126=0.0, V127=0.0, V128=0.0, V129=0.0, V130=0.0, V131=0.0, V132=0.0, V133=0.0, V134=0.0, V135=0.0, V136=0.0, V137=0.0, V138=None, V139=None, V140=None, V141=None, V142=None, V143=None, V144=None, V145=None, V146=None, V147=None, V148=None, V149=None, V150=None, V151=None, V152=None, V153=None, V154=None, V155=None, V156=None, V157=None, V158=None, V159=None, V160=None, V161=None, V162=None, V163=None, V164=None, V165=None, V166=None, V167=0.0, V168=0.0, V169=0.0, V170=1.0, V171=1.0, V172=0.0, V173=0.0, V174=0.0, V175=0.0, V176=1.0, V177=0.0, V178=0.0, V179=0.0, V180=0.0, V181=0.0, V182=0.0, V183=0.0, V184=0.0, V185=0.0, V186=1.0, V187=1.0, V188=1.0, V189=1.0, V190=1.0, V191=1.0, V192=1.0, V193=1.0, V194=1.0, V195=1.0, V196=1.0, V197=1.0, V198=1.0, V199=1.0, V200=1.0, V201=1.0, V202=0.0, V203=0.0, V204=0.0, V205=0.0, V206=0.0, V207=0.0, V208=0.0, V209=0.0, V210=0.0, V211=0.0, V212=0.0, V213=0.0, V214=0.0, V215=0.0, V216=0.0, V217=0.0, V218=0.0, V219=0.0, V220=0.0, V221=1.0, V222=1.0, V223=0.0, V224=0.0, V225=0.0, V226=0.0, V227=0.0, V228=1.0, V229=1.0, V230=1.0, V231=0.0, V232=0.0, V233=0.0, V234=0.0, V235=0.0, V236=0.0, V237=0.0, V238=0.0, V239=0.0, V240=1.0, V241=1.0, V242=1.0, V243=1.0, V244=1.0, V245=1.0, V246=1.0, V247=1.0, V248=1.0, V249=1.0, V250=1.0, V251=1.0, V252=1.0, V253=1.0, V254=1.0, V255=1.0, V256=1.0, V257=1.0, V258=1.0, V259=1.0, V260=1.0, V261=1.0, V262=1.0, V263=0.0, V264=0.0, V265=0.0, V266=0.0, V267=0.0, V268=0.0, V269=0.0, V270=0.0, V271=0.0, V272=0.0, V273=0.0, V274=0.0, V275=0.0, V276=0.0, V277=0.0, V278=0.0, V279=0.0, V280=0.0, V281=0.0, V282=1.0, V283=1.0, V284=0.0, V285=0.0, V286=0.0, V287=0.0, V288=0.0, V289=0.0, V290=1.0, V291=1.0, V292=1.0, V293=0.0, V294=0.0, V295=0.0, V296=0.0, V297=0.0, V298=0.0, V299=0.0, V300=0.0, V301=0.0, V302=1.0, V303=1.0, V304=1.0, V305=1.0, V306=0.0, V307=0.0, V308=0.0, V309=0.0, V310=0.0, V311=0.0, V312=0.0, V313=0.0, V314=0.0, V315=0.0, V316=0.0, V317=0.0, V318=0.0, V319=0.0, V320=0.0, V321=0.0, V322=None, V323=None, V324=None, V325=None, V326=None, V327=None, V328=None, V329=None, V330=None, V331=None, V332=None, V333=None, V334=None, V335=None, V336=None, V337=None, V338=None, V339=None, id_01=-5.0, id_02=257037.0, id_03=None, id_04=None, id_05=0.0, id_06=0.0, id_07=None, id_08=None, id_09=None, id_10=None, id_11=100.0, id_12='NotFound', id_13=52.0, id_14=None, id_15='New', id_16='NotFound', id_17=225.0, id_18=None, id_19=484.0, id_20=507.0, id_21=None, id_22=None, id_23=None, id_24=None, id_25=None, id_26=None, id_27=None, id_28='New', id_29='NotFound', id_30=None, id_31='chrome 62.0', id_32=None, id_33=None, id_34=None, id_35='F', id_36='F', id_37='T', id_38='T', DeviceType='desktop', DeviceInfo='Windows')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.count())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_count = train.count()\n",
    "def null_percentage(df, column):\n",
    "    null_count = df.where(col(column).isNull()).count()\n",
    "    return null_count / total_count\n",
    "\n",
    " \n",
    "for column in train.columns:\n",
    "    if null_percentage(train, column) > 0.05:\n",
    "        train = train.drop(col(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1759:===================================>                  (13 + 7) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144233, 194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((train.count(), len(train.columns)))\n",
    "train.toPandas().to_csv('./data/traincleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from train cleaned file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = spark_object.read.option(\"delimiter\", \",\").csv(\"data/train_combined.csv\", header='true', inferSchema='true')\n",
    "test = spark_object.read.option(\"delimiter\", \",\").csv(\"data/test_combined.csv\", header='true', inferSchema='true')\n",
    "\n",
    "print((train.count(),len(train.columns)))\n",
    "print((test.count(),len(test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_missing_cols(train, n=None, thresh=80):\n",
    "#     # Convert the DataFrame to Pandas and process it\n",
    "    \n",
    "    \n",
    "#     dff = dff.toPandas()\n",
    "#     dff = dff.melt(var_name='col', value_name='missing_percent')\n",
    "#     dff = dff.sort_values(by=['missing_percent'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     print(f'There are {total_cols} columns in this dataset.')\n",
    "#     print(f'There are {dff[dff[\"missing_percent\"] > thresh].shape[0]} columns with missing percent values than {thresh}%')\n",
    "    \n",
    "#     # Return the top missing columns\n",
    "#     if n:\n",
    "#         return dff.head(n)\n",
    "#     else:\n",
    "#         return dff\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 434 columns in this dataset.\n",
      "There are 214 columns with missing percent values than 50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_missing = top_missing_cols(train, n=None, thresh=50)\n",
    "# missing_cols = df_missing['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "NAN count = 585793 percent: 99.19615944728554 %\n",
      "['id_24']\n",
      "####################\n",
      "NAN count = 585408 percent: 99.13096487960172 %\n",
      "['id_25']\n",
      "####################\n",
      "NAN count = 585385 percent: 99.12707013919464 %\n",
      "['id_07', 'id_08']\n",
      "####################\n",
      "NAN count = 585381 percent: 99.12639279303687 %\n",
      "['id_21']\n",
      "####################\n",
      "NAN count = 585377 percent: 99.12571544687913 %\n",
      "['id_26']\n",
      "####################\n",
      "NAN count = 585371 percent: 99.1246994276425 %\n",
      "['id_27', 'id_23', 'id_22']\n",
      "####################\n",
      "NAN count = 552913 percent: 93.62837403054831 %\n",
      "['dist2']\n",
      "####################\n",
      "NAN count = 551623 percent: 93.40992989467267 %\n",
      "['D7']\n",
      "####################\n",
      "NAN count = 545427 percent: 92.36072069631184 %\n",
      "['id_18']\n",
      "####################\n",
      "NAN count = 528588 percent: 89.50926270870728 %\n",
      "['D13']\n",
      "####################\n",
      "NAN count = 528353 percent: 89.46946862193924 %\n",
      "['D14']\n",
      "####################\n",
      "NAN count = 525823 percent: 89.04104717715988 %\n",
      "['D12']\n",
      "####################\n",
      "NAN count = 524216 percent: 88.76892335828225 %\n",
      "['id_03', 'id_04']\n",
      "####################\n",
      "NAN count = 517353 percent: 87.60676668811597 %\n",
      "['D6']\n",
      "####################\n",
      "NAN count = 517251 percent: 87.58949436109323 %\n",
      "['id_33']\n",
      "####################\n",
      "NAN count = 515614 percent: 87.31229044603245 %\n",
      "['id_10', 'id_09', 'D9', 'D8']\n",
      "####################\n",
      "NAN count = 512975 percent: 86.8654113184543 %\n",
      "['id_30']\n",
      "####################\n",
      "NAN count = 512954 percent: 86.86185525112609 %\n",
      "['id_32']\n",
      "####################\n",
      "NAN count = 512735 percent: 86.82477054898906 %\n",
      "['id_34']\n",
      "####################\n",
      "NAN count = 510496 percent: 86.4456260371863 %\n",
      "['id_14']\n",
      "####################\n",
      "NAN count = 508595 percent: 86.12371727571374 %\n",
      "['V142', 'V158', 'V140', 'V162', 'V141', 'V161', 'V157', 'V146', 'V156', 'V155', 'V154', 'V153', 'V149', 'V147', 'V148', 'V163', 'V139', 'V138']\n",
      "####################\n",
      "NAN count = 508589 percent: 86.12270125647711 %\n",
      "['V160', 'V151', 'V152', 'V145', 'V144', 'V143', 'V159', 'V164', 'V165', 'V166', 'V150']\n",
      "####################\n",
      "NAN count = 508189 percent: 86.05496664070174 %\n",
      "['V337', 'V333', 'V336', 'V335', 'V334', 'V338', 'V339', 'V324', 'V332', 'V325', 'V330', 'V329', 'V328', 'V327', 'V326', 'V322', 'V323', 'V331']\n",
      "####################\n",
      "NAN count = 471874 percent: 79.90551021099333 %\n",
      "['DeviceInfo']\n",
      "####################\n",
      "NAN count = 463220 percent: 78.44007179869273 %\n",
      "['id_13']\n",
      "####################\n",
      "NAN count = 461200 percent: 78.098011989027 %\n",
      "['id_16']\n",
      "####################\n",
      "NAN count = 460110 percent: 77.91343516103905 %\n",
      "['V278', 'V277', 'V252', 'V253', 'V254', 'V257', 'V258', 'V242', 'V261', 'V262', 'V263', 'V264', 'V249', 'V266', 'V267', 'V268', 'V269', 'V273', 'V274', 'V275', 'V276', 'V265', 'V260', 'V247', 'V246', 'V240', 'V237', 'V236', 'V235', 'V233', 'V232', 'V231', 'V230', 'V229', 'V228', 'V226', 'V225', 'V224', 'V223', 'V219', 'V218', 'V217', 'V243', 'V244', 'V248', 'V241']\n",
      "####################\n",
      "NAN count = 453675 percent: 76.82375452975243 %\n",
      "['id_05', 'id_06']\n",
      "####################\n",
      "NAN count = 453249 percent: 76.75161716395164 %\n",
      "['R_emaildomain']\n",
      "####################\n",
      "NAN count = 451279 percent: 76.41802418125782 %\n",
      "['id_20']\n",
      "####################\n",
      "NAN count = 451222 percent: 76.40837199850984 %\n",
      "['id_19']\n",
      "####################\n",
      "NAN count = 451171 percent: 76.39973583499847 %\n",
      "['id_17']\n",
      "####################\n",
      "NAN count = 450909 percent: 76.35536966166559 %\n",
      "['V212', 'V211', 'V214', 'V213', 'V196', 'V205', 'V183', 'V216', 'V206', 'V186', 'V187', 'V192', 'V207', 'V215', 'V181', 'V182', 'V191', 'V167', 'V168', 'V199', 'V193', 'V172', 'V173', 'V202', 'V203', 'V176', 'V177', 'V178', 'V179', 'V204', 'V190']\n",
      "####################\n",
      "NAN count = 450721 percent: 76.32353439225116 %\n",
      "['V194', 'V200', 'V189', 'V188', 'V185', 'V184', 'V180', 'V175', 'V174', 'V171', 'V170', 'V169', 'V195', 'V201', 'V197', 'V198', 'V209', 'V208', 'V210']\n",
      "####################\n",
      "NAN count = 450258 percent: 76.24513157449114 %\n",
      "['id_31']\n",
      "####################\n",
      "NAN count = 449730 percent: 76.15572188166763 %\n",
      "['DeviceType']\n",
      "####################\n",
      "NAN count = 449668 percent: 76.14522301622245 %\n",
      "['id_02']\n",
      "####################\n",
      "NAN count = 449562 percent: 76.12727334304196 %\n",
      "['id_29', 'id_11', 'id_28']\n",
      "####################\n",
      "NAN count = 449555 percent: 76.12608798726589 %\n",
      "['id_37', 'id_36', 'id_15', 'id_35', 'id_38']\n",
      "####################\n",
      "NAN count = 449124 percent: 76.0531039387679 %\n",
      "['V245', 'V271', 'V234', 'V222', 'V238', 'V239', 'V227', 'V250', 'V272', 'V270', 'V251', 'V220', 'V255', 'V256', 'V259', 'V221']\n",
      "####################\n",
      "NAN count = 446307 percent: 75.5760829071697 %\n",
      "['id_01', 'id_12']\n",
      "####################\n",
      "NAN count = 352271 percent: 59.6523520845328 %\n",
      "['dist1']\n",
      "####################\n",
      "NAN count = 350482 percent: 59.34940901547736 %\n",
      "['M5']\n",
      "####################\n",
      "NAN count = 346265 percent: 58.63531682866528 %\n",
      "['M7']\n",
      "####################\n",
      "NAN count = 346252 percent: 58.633115453652586 %\n",
      "['M9', 'M8']\n",
      "####################\n",
      "NAN count = 309841 percent: 52.4674027161581 %\n",
      "['D5']\n",
      "####################\n",
      "NAN count = 281444 percent: 47.658753005723575 %\n",
      "['M4']\n",
      "####################\n",
      "NAN count = 280797 percent: 47.54919226470688 %\n",
      "['D2']\n",
      "####################\n",
      "NAN count = 279287 percent: 47.29349409015477 %\n",
      "['V3', 'V9', 'V5', 'V11', 'V10', 'V8', 'V7', 'D11', 'V6', 'V4', 'V2', 'V1']\n",
      "####################\n",
      "NAN count = 271100 percent: 45.90713584177194 %\n",
      "['M2', 'M3', 'M1']\n",
      "####################\n",
      "NAN count = 262878 percent: 44.514850814508755 %\n",
      "['D3']\n",
      "####################\n",
      "NAN count = 169360 percent: 28.678836319300977 %\n",
      "['M6']\n",
      "####################\n",
      "NAN count = 168969 percent: 28.612625732380533 %\n",
      "['V35', 'V40', 'V41', 'V39', 'V38', 'V51', 'V37', 'V52', 'V36', 'V50', 'V48', 'V42', 'V43', 'V44', 'V46', 'V47', 'V45', 'V49']\n",
      "####################\n",
      "NAN count = 168922 percent: 28.60466691502693 %\n",
      "['D4']\n",
      "####################\n",
      "NAN count = 94456 percent: 15.99485216920107 %\n",
      "['P_emaildomain']\n",
      "####################\n",
      "NAN count = 89164 percent: 15.098723202492634 %\n",
      "['V80', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V86', 'V79', 'V85', 'V75', 'V84', 'V77', 'V83', 'V78', 'V82', 'V81', 'V76']\n",
      "####################\n",
      "NAN count = 89113 percent: 15.09008703898127 %\n",
      "['D15']\n",
      "####################\n",
      "NAN count = 77096 percent: 13.055169844549056 %\n",
      "['V72', 'V74', 'V73', 'V71', 'V65', 'V68', 'V58', 'V70', 'V53', 'V54', 'V55', 'V56', 'V57', 'V59', 'V67', 'V60', 'V61', 'V62', 'V63', 'V64', 'V66', 'V69']\n",
      "####################\n",
      "NAN count = 76073 percent: 12.881938564703491 %\n",
      "['V21', 'V22', 'V23', 'V34', 'V33', 'V32', 'V31', 'V30', 'V29', 'V28', 'V27', 'V25', 'V24', 'V26', 'V16', 'V15', 'V20', 'V14', 'V19', 'V18', 'V17', 'V12', 'V13']\n",
      "####################\n",
      "NAN count = 76022 percent: 12.873302401192129 %\n",
      "['D10']\n",
      "####################\n",
      "NAN count = 65706 percent: 11.12642666034477 %\n",
      "['addr1', 'addr2']\n",
      "####################\n",
      "NAN count = 8933 percent: 1.5126833068039423 %\n",
      "['card2']\n",
      "####################\n",
      "NAN count = 4259 percent: 0.7212043214684865 %\n",
      "['card5']\n",
      "####################\n",
      "NAN count = 1577 percent: 0.267043722694483 %\n",
      "['card4']\n",
      "####################\n",
      "NAN count = 1571 percent: 0.26602770345785215 %\n",
      "['card6']\n",
      "####################\n",
      "NAN count = 1565 percent: 0.26501168422122123 %\n",
      "['card3']\n",
      "####################\n",
      "NAN count = 1269 percent: 0.21488806854743114 %\n",
      "['V296', 'V289', 'V288', 'V283', 'V282', 'V281', 'V300', 'V301', 'V313', 'V314', 'V315', 'D1']\n",
      "####################\n",
      "NAN count = 314 percent: 0.053171673383682734 %\n",
      "['V104', 'V109', 'V110', 'V111', 'V112', 'V106', 'V105', 'V102', 'V103', 'V96', 'V101', 'V100', 'V99', 'V98', 'V97', 'V95', 'V135', 'V134', 'V107', 'V133', 'V132', 'V131', 'V130', 'V129', 'V128', 'V127', 'V126', 'V125', 'V124', 'V123', 'V122', 'V121', 'V120', 'V119', 'V118', 'V117', 'V116', 'V115', 'V114', 'V113', 'V136', 'V137', 'V108']\n",
      "####################\n",
      "NAN count = 12 percent: 0.0020320384732617604 %\n",
      "['V311', 'V321', 'V294', 'V306', 'V305', 'V304', 'V303', 'V302', 'V299', 'V298', 'V297', 'V295', 'V293', 'V308', 'V292', 'V291', 'V290', 'V287', 'V286', 'V285', 'V284', 'V280', 'V279', 'V320', 'V307', 'V309', 'V312', 'V316', 'V317', 'V318', 'V319', 'V310']\n",
      "####################\n",
      "NAN count = 0 percent: 0.0 %\n",
      "['C9', 'C14', 'C13', 'C12', 'C11', 'C10', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'C6', 'card1', 'C8', 'C7', 'C5', 'C4', 'C3', 'C2', 'C1', 'isFraud', 'TransactionID']\n"
     ]
    }
   ],
   "source": [
    "# nan_dict = {}\n",
    "# for col_name in missing_cols:\n",
    "#     count_val = train.select([count(when(col(c).isNull(), c)).alias(c) for c in [col_name]]).collect()[0][0]\n",
    "#     try:\n",
    "#         nan_dict[count_val].append(col_name)\n",
    "#     except:\n",
    "#         nan_dict[count_val] = [col_name]\n",
    "\n",
    "# for k,v in nan_dict.items():\n",
    "#     print(f'#####' * 4)\n",
    "#     print(f'NAN count = {k} percent: {(int(k)/train.count())*100} %')\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = (['ProductCD'] +\n",
    "            ['card{}'.format(i) for i in range(1, 7)] +\n",
    "            ['addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] +\n",
    "            ['M{}'.format(i) for i in range(1, 10)] +\n",
    "            ['DeviceType', 'DeviceInfo'] +\n",
    "            ['id_{}'.format(i) for i in range(12, 39)])\n",
    "\n",
    "type_map = {c: 'string' for c in cat_cols}\n",
    "\n",
    "for c in cat_cols:\n",
    "    train = train.withColumn(c, col(c).cast('string'))\n",
    "    test = test.withColumn(c, col(c).cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_cols = ['TransactionID', 'TransactionDT']\n",
    "target = 'isFraud'\n",
    "\n",
    "numeric_cols =  [\n",
    "    'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', \n",
    "    'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', \n",
    "    'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', \n",
    "    'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', \n",
    "    'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', \n",
    "    'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', \n",
    "    'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', \n",
    "    'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', \n",
    "    'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', \n",
    "    'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', \n",
    "    'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', \n",
    "    'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', \n",
    "    'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', \n",
    "    'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', \n",
    "    'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', \n",
    "    'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', \n",
    "    'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', \n",
    "    'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', \n",
    "    'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', \n",
    "    'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', \n",
    "    'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', \n",
    "    'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', \n",
    "    'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', \n",
    "    'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', \n",
    "    'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', \n",
    "    'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', \n",
    "    'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', \n",
    "    'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', \n",
    "    'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', \n",
    "    'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', \n",
    "    'id_09', 'id_10', 'id_11'\n",
    "]\n",
    "\n",
    "\n",
    "reduced_vcols = ['V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17', 'V20', \n",
    " 'V23', 'V26', 'V27', 'V30', 'V36', 'V37', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V56', 'V59', \n",
    " 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86', 'V88', 'V89', 'V91', 'V96', \n",
    " 'V98', 'V99', 'V104', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121', 'V123', 'V124', 'V127', \n",
    " 'V129', 'V130', 'V136', 'V138', 'V139', 'V142', 'V147', 'V156', 'V162', 'V165', 'V160', 'V166', 'V178',\n",
    " 'V176', 'V173', 'V182', 'V187', 'V203', 'V205', 'V207', 'V215', 'V169', 'V171', 'V175', 'V180', 'V185', \n",
    " 'V188', 'V198', 'V210', 'V209', 'V218', 'V223', 'V224', 'V226', 'V228', 'V229', 'V235', 'V240', 'V258', \n",
    " 'V257', 'V253', 'V252', 'V260', 'V261', 'V264', 'V266', 'V267', 'V274', 'V277', 'V220', 'V221', 'V234', \n",
    " 'V238', 'V250', 'V271', 'V294', 'V284', 'V285', 'V286', 'V291',\n",
    " 'V297', 'V303', 'V305', 'V307', 'V309', 'V310', 'V320', 'V281', 'V283', 'V289', 'V296', 'V301', 'V314', 'V332', 'V325', 'V335', 'V338']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 211 columns\n"
     ]
    }
   ],
   "source": [
    "drop_cols = [col for col in train.columns if col.startswith('V') and col not in reduced_vcols]\n",
    "\n",
    "print(f'dropping {len(drop_cols)} columns')\n",
    "df_train = train.drop(*drop_cols)\n",
    "df_test = test.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.select(col('isFraud'))\n",
    "X_train = df_train.select([col for col in df_train.columns if col != 'isFraud'])\n",
    "X_test = df_test.select([col for col in df_test.columns])\n",
    "\n",
    "# print(X_train.count(), len(X_train.columns))\n",
    "# print(X_test.count(), len(X_test.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "for col in X_train.columns:\n",
    "    if col in cat_cols:\n",
    "        indexer = StringIndexer(inputCol=col, outputCol=col+\"_indexed\")\n",
    "        indexer.setHandleInvalid(\"keep\")\n",
    "        model = indexer.fit(X_train)\n",
    "        X_train = model.transform(X_train).drop(col)\n",
    "        X_train = X_train.withColumnRenamed(col+\"_indexed\", col)\n",
    "\n",
    "        X_test = model.transform(X_test).drop(col)\n",
    "        X_test = X_test.withColumnRenamed(col+\"_indexed\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_cols = []\n",
    "rem_cols.extend(['TransactionDT','TransactionID'])\n",
    "\n",
    "cols = [col for col in X_train.columns if col not in rem_cols]\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: integer (nullable = true)\n",
      " |-- TransactionDT: integer (nullable = true)\n",
      " |-- TransactionAmt: double (nullable = true)\n",
      " |-- dist1: double (nullable = true)\n",
      " |-- dist2: double (nullable = true)\n",
      " |-- C1: double (nullable = true)\n",
      " |-- C2: double (nullable = true)\n",
      " |-- C3: double (nullable = true)\n",
      " |-- C4: double (nullable = true)\n",
      " |-- C5: double (nullable = true)\n",
      " |-- C6: double (nullable = true)\n",
      " |-- C7: double (nullable = true)\n",
      " |-- C8: double (nullable = true)\n",
      " |-- C9: double (nullable = true)\n",
      " |-- C10: double (nullable = true)\n",
      " |-- C11: double (nullable = true)\n",
      " |-- C12: double (nullable = true)\n",
      " |-- C13: double (nullable = true)\n",
      " |-- C14: double (nullable = true)\n",
      " |-- D1: double (nullable = true)\n",
      " |-- D2: double (nullable = true)\n",
      " |-- D3: double (nullable = true)\n",
      " |-- D4: double (nullable = true)\n",
      " |-- D5: double (nullable = true)\n",
      " |-- D6: double (nullable = true)\n",
      " |-- D7: double (nullable = true)\n",
      " |-- D8: double (nullable = true)\n",
      " |-- D9: double (nullable = true)\n",
      " |-- D10: double (nullable = true)\n",
      " |-- D11: double (nullable = true)\n",
      " |-- D12: double (nullable = true)\n",
      " |-- D13: double (nullable = true)\n",
      " |-- D14: double (nullable = true)\n",
      " |-- D15: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V30: double (nullable = true)\n",
      " |-- V36: double (nullable = true)\n",
      " |-- V37: double (nullable = true)\n",
      " |-- V40: double (nullable = true)\n",
      " |-- V41: double (nullable = true)\n",
      " |-- V44: double (nullable = true)\n",
      " |-- V47: double (nullable = true)\n",
      " |-- V48: double (nullable = true)\n",
      " |-- V54: double (nullable = true)\n",
      " |-- V56: double (nullable = true)\n",
      " |-- V59: double (nullable = true)\n",
      " |-- V62: double (nullable = true)\n",
      " |-- V65: double (nullable = true)\n",
      " |-- V67: double (nullable = true)\n",
      " |-- V68: double (nullable = true)\n",
      " |-- V70: double (nullable = true)\n",
      " |-- V76: double (nullable = true)\n",
      " |-- V78: double (nullable = true)\n",
      " |-- V80: double (nullable = true)\n",
      " |-- V82: double (nullable = true)\n",
      " |-- V86: double (nullable = true)\n",
      " |-- V88: double (nullable = true)\n",
      " |-- V89: double (nullable = true)\n",
      " |-- V91: double (nullable = true)\n",
      " |-- V96: double (nullable = true)\n",
      " |-- V98: double (nullable = true)\n",
      " |-- V99: double (nullable = true)\n",
      " |-- V104: double (nullable = true)\n",
      " |-- V107: double (nullable = true)\n",
      " |-- V108: double (nullable = true)\n",
      " |-- V111: double (nullable = true)\n",
      " |-- V115: double (nullable = true)\n",
      " |-- V117: double (nullable = true)\n",
      " |-- V120: double (nullable = true)\n",
      " |-- V121: double (nullable = true)\n",
      " |-- V123: double (nullable = true)\n",
      " |-- V124: double (nullable = true)\n",
      " |-- V127: double (nullable = true)\n",
      " |-- V129: double (nullable = true)\n",
      " |-- V130: double (nullable = true)\n",
      " |-- V136: double (nullable = true)\n",
      " |-- V138: double (nullable = true)\n",
      " |-- V139: double (nullable = true)\n",
      " |-- V142: double (nullable = true)\n",
      " |-- V147: double (nullable = true)\n",
      " |-- V156: double (nullable = true)\n",
      " |-- V160: double (nullable = true)\n",
      " |-- V162: double (nullable = true)\n",
      " |-- V165: double (nullable = true)\n",
      " |-- V166: double (nullable = true)\n",
      " |-- V169: double (nullable = true)\n",
      " |-- V171: double (nullable = true)\n",
      " |-- V173: double (nullable = true)\n",
      " |-- V175: double (nullable = true)\n",
      " |-- V176: double (nullable = true)\n",
      " |-- V178: double (nullable = true)\n",
      " |-- V180: double (nullable = true)\n",
      " |-- V182: double (nullable = true)\n",
      " |-- V185: double (nullable = true)\n",
      " |-- V187: double (nullable = true)\n",
      " |-- V188: double (nullable = true)\n",
      " |-- V198: double (nullable = true)\n",
      " |-- V203: double (nullable = true)\n",
      " |-- V205: double (nullable = true)\n",
      " |-- V207: double (nullable = true)\n",
      " |-- V209: double (nullable = true)\n",
      " |-- V210: double (nullable = true)\n",
      " |-- V215: double (nullable = true)\n",
      " |-- V218: double (nullable = true)\n",
      " |-- V220: double (nullable = true)\n",
      " |-- V221: double (nullable = true)\n",
      " |-- V223: double (nullable = true)\n",
      " |-- V224: double (nullable = true)\n",
      " |-- V226: double (nullable = true)\n",
      " |-- V228: double (nullable = true)\n",
      " |-- V229: double (nullable = true)\n",
      " |-- V234: double (nullable = true)\n",
      " |-- V235: double (nullable = true)\n",
      " |-- V238: double (nullable = true)\n",
      " |-- V240: double (nullable = true)\n",
      " |-- V250: double (nullable = true)\n",
      " |-- V252: double (nullable = true)\n",
      " |-- V253: double (nullable = true)\n",
      " |-- V257: double (nullable = true)\n",
      " |-- V258: double (nullable = true)\n",
      " |-- V260: double (nullable = true)\n",
      " |-- V261: double (nullable = true)\n",
      " |-- V264: double (nullable = true)\n",
      " |-- V266: double (nullable = true)\n",
      " |-- V267: double (nullable = true)\n",
      " |-- V271: double (nullable = true)\n",
      " |-- V274: double (nullable = true)\n",
      " |-- V277: double (nullable = true)\n",
      " |-- V281: double (nullable = true)\n",
      " |-- V283: double (nullable = true)\n",
      " |-- V284: double (nullable = true)\n",
      " |-- V285: double (nullable = true)\n",
      " |-- V286: double (nullable = true)\n",
      " |-- V289: double (nullable = true)\n",
      " |-- V291: double (nullable = true)\n",
      " |-- V294: double (nullable = true)\n",
      " |-- V296: double (nullable = true)\n",
      " |-- V297: double (nullable = true)\n",
      " |-- V301: double (nullable = true)\n",
      " |-- V303: double (nullable = true)\n",
      " |-- V305: double (nullable = true)\n",
      " |-- V307: double (nullable = true)\n",
      " |-- V309: double (nullable = true)\n",
      " |-- V310: double (nullable = true)\n",
      " |-- V314: double (nullable = true)\n",
      " |-- V320: double (nullable = true)\n",
      " |-- V325: double (nullable = true)\n",
      " |-- V332: double (nullable = true)\n",
      " |-- V335: double (nullable = true)\n",
      " |-- V338: double (nullable = true)\n",
      " |-- id_01: double (nullable = true)\n",
      " |-- id_02: double (nullable = true)\n",
      " |-- id_03: double (nullable = true)\n",
      " |-- id_04: double (nullable = true)\n",
      " |-- id_05: double (nullable = true)\n",
      " |-- id_06: double (nullable = true)\n",
      " |-- id_07: double (nullable = true)\n",
      " |-- id_08: double (nullable = true)\n",
      " |-- id_09: double (nullable = true)\n",
      " |-- id_10: double (nullable = true)\n",
      " |-- id_11: double (nullable = true)\n",
      " |-- ProductCD: double (nullable = false)\n",
      " |-- card1: double (nullable = false)\n",
      " |-- card2: double (nullable = false)\n",
      " |-- card3: double (nullable = false)\n",
      " |-- card4: double (nullable = false)\n",
      " |-- card5: double (nullable = false)\n",
      " |-- card6: double (nullable = false)\n",
      " |-- addr1: double (nullable = false)\n",
      " |-- addr2: double (nullable = false)\n",
      " |-- P_emaildomain: double (nullable = false)\n",
      " |-- R_emaildomain: double (nullable = false)\n",
      " |-- M1: double (nullable = false)\n",
      " |-- M2: double (nullable = false)\n",
      " |-- M3: double (nullable = false)\n",
      " |-- M4: double (nullable = false)\n",
      " |-- M5: double (nullable = false)\n",
      " |-- M6: double (nullable = false)\n",
      " |-- M7: double (nullable = false)\n",
      " |-- M8: double (nullable = false)\n",
      " |-- M9: double (nullable = false)\n",
      " |-- id_12: double (nullable = false)\n",
      " |-- id_13: double (nullable = false)\n",
      " |-- id_14: double (nullable = false)\n",
      " |-- id_15: double (nullable = false)\n",
      " |-- id_16: double (nullable = false)\n",
      " |-- id_17: double (nullable = false)\n",
      " |-- id_18: double (nullable = false)\n",
      " |-- id_19: double (nullable = false)\n",
      " |-- id_20: double (nullable = false)\n",
      " |-- id_21: double (nullable = false)\n",
      " |-- id_22: double (nullable = false)\n",
      " |-- id_23: double (nullable = false)\n",
      " |-- id_24: double (nullable = false)\n",
      " |-- id_25: double (nullable = false)\n",
      " |-- id_26: double (nullable = false)\n",
      " |-- id_27: double (nullable = false)\n",
      " |-- id_28: double (nullable = false)\n",
      " |-- id_29: double (nullable = false)\n",
      " |-- id_30: double (nullable = false)\n",
      " |-- id_31: double (nullable = false)\n",
      " |-- id_32: double (nullable = false)\n",
      " |-- id_33: double (nullable = false)\n",
      " |-- id_34: double (nullable = false)\n",
      " |-- id_35: double (nullable = false)\n",
      " |-- id_36: double (nullable = false)\n",
      " |-- id_37: double (nullable = false)\n",
      " |-- id_38: double (nullable = false)\n",
      " |-- DeviceType: double (nullable = false)\n",
      " |-- DeviceInfo: double (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "root\n",
      " |-- TransactionID: integer (nullable = true)\n",
      " |-- TransactionDT: integer (nullable = true)\n",
      " |-- TransactionAmt: vector (nullable = true)\n",
      " |-- dist1: vector (nullable = true)\n",
      " |-- dist2: vector (nullable = true)\n",
      " |-- C1: vector (nullable = true)\n",
      " |-- C2: vector (nullable = true)\n",
      " |-- C3: vector (nullable = true)\n",
      " |-- C4: vector (nullable = true)\n",
      " |-- C5: vector (nullable = true)\n",
      " |-- C6: vector (nullable = true)\n",
      " |-- C7: vector (nullable = true)\n",
      " |-- C8: vector (nullable = true)\n",
      " |-- C9: vector (nullable = true)\n",
      " |-- C10: vector (nullable = true)\n",
      " |-- C11: vector (nullable = true)\n",
      " |-- C12: vector (nullable = true)\n",
      " |-- C13: vector (nullable = true)\n",
      " |-- C14: vector (nullable = true)\n",
      " |-- D1: vector (nullable = true)\n",
      " |-- D2: vector (nullable = true)\n",
      " |-- D3: vector (nullable = true)\n",
      " |-- D4: vector (nullable = true)\n",
      " |-- D5: vector (nullable = true)\n",
      " |-- D6: vector (nullable = true)\n",
      " |-- D7: vector (nullable = true)\n",
      " |-- D8: vector (nullable = true)\n",
      " |-- D9: vector (nullable = true)\n",
      " |-- D10: vector (nullable = true)\n",
      " |-- D11: vector (nullable = true)\n",
      " |-- D12: vector (nullable = true)\n",
      " |-- D13: vector (nullable = true)\n",
      " |-- D14: vector (nullable = true)\n",
      " |-- D15: vector (nullable = true)\n",
      " |-- V1: vector (nullable = true)\n",
      " |-- V3: vector (nullable = true)\n",
      " |-- V4: vector (nullable = true)\n",
      " |-- V6: vector (nullable = true)\n",
      " |-- V8: vector (nullable = true)\n",
      " |-- V11: vector (nullable = true)\n",
      " |-- V13: vector (nullable = true)\n",
      " |-- V14: vector (nullable = true)\n",
      " |-- V17: vector (nullable = true)\n",
      " |-- V20: vector (nullable = true)\n",
      " |-- V23: vector (nullable = true)\n",
      " |-- V26: vector (nullable = true)\n",
      " |-- V27: vector (nullable = true)\n",
      " |-- V30: vector (nullable = true)\n",
      " |-- V36: vector (nullable = true)\n",
      " |-- V37: vector (nullable = true)\n",
      " |-- V40: vector (nullable = true)\n",
      " |-- V41: vector (nullable = true)\n",
      " |-- V44: vector (nullable = true)\n",
      " |-- V47: vector (nullable = true)\n",
      " |-- V48: vector (nullable = true)\n",
      " |-- V54: vector (nullable = true)\n",
      " |-- V56: vector (nullable = true)\n",
      " |-- V59: vector (nullable = true)\n",
      " |-- V62: vector (nullable = true)\n",
      " |-- V65: vector (nullable = true)\n",
      " |-- V67: vector (nullable = true)\n",
      " |-- V68: vector (nullable = true)\n",
      " |-- V70: vector (nullable = true)\n",
      " |-- V76: vector (nullable = true)\n",
      " |-- V78: vector (nullable = true)\n",
      " |-- V80: vector (nullable = true)\n",
      " |-- V82: vector (nullable = true)\n",
      " |-- V86: vector (nullable = true)\n",
      " |-- V88: vector (nullable = true)\n",
      " |-- V89: vector (nullable = true)\n",
      " |-- V91: vector (nullable = true)\n",
      " |-- V96: vector (nullable = true)\n",
      " |-- V98: vector (nullable = true)\n",
      " |-- V99: vector (nullable = true)\n",
      " |-- V104: vector (nullable = true)\n",
      " |-- V107: vector (nullable = true)\n",
      " |-- V108: vector (nullable = true)\n",
      " |-- V111: vector (nullable = true)\n",
      " |-- V115: vector (nullable = true)\n",
      " |-- V117: vector (nullable = true)\n",
      " |-- V120: vector (nullable = true)\n",
      " |-- V121: vector (nullable = true)\n",
      " |-- V123: vector (nullable = true)\n",
      " |-- V124: vector (nullable = true)\n",
      " |-- V127: vector (nullable = true)\n",
      " |-- V129: vector (nullable = true)\n",
      " |-- V130: vector (nullable = true)\n",
      " |-- V136: vector (nullable = true)\n",
      " |-- V138: vector (nullable = true)\n",
      " |-- V139: vector (nullable = true)\n",
      " |-- V142: vector (nullable = true)\n",
      " |-- V147: vector (nullable = true)\n",
      " |-- V156: vector (nullable = true)\n",
      " |-- V160: vector (nullable = true)\n",
      " |-- V162: vector (nullable = true)\n",
      " |-- V165: vector (nullable = true)\n",
      " |-- V166: vector (nullable = true)\n",
      " |-- V169: vector (nullable = true)\n",
      " |-- V171: vector (nullable = true)\n",
      " |-- V173: vector (nullable = true)\n",
      " |-- V175: vector (nullable = true)\n",
      " |-- V176: vector (nullable = true)\n",
      " |-- V178: vector (nullable = true)\n",
      " |-- V180: vector (nullable = true)\n",
      " |-- V182: vector (nullable = true)\n",
      " |-- V185: vector (nullable = true)\n",
      " |-- V187: vector (nullable = true)\n",
      " |-- V188: vector (nullable = true)\n",
      " |-- V198: vector (nullable = true)\n",
      " |-- V203: vector (nullable = true)\n",
      " |-- V205: vector (nullable = true)\n",
      " |-- V207: vector (nullable = true)\n",
      " |-- V209: vector (nullable = true)\n",
      " |-- V210: vector (nullable = true)\n",
      " |-- V215: vector (nullable = true)\n",
      " |-- V218: vector (nullable = true)\n",
      " |-- V220: vector (nullable = true)\n",
      " |-- V221: vector (nullable = true)\n",
      " |-- V223: vector (nullable = true)\n",
      " |-- V224: vector (nullable = true)\n",
      " |-- V226: vector (nullable = true)\n",
      " |-- V228: vector (nullable = true)\n",
      " |-- V229: vector (nullable = true)\n",
      " |-- V234: vector (nullable = true)\n",
      " |-- V235: vector (nullable = true)\n",
      " |-- V238: vector (nullable = true)\n",
      " |-- V240: vector (nullable = true)\n",
      " |-- V250: vector (nullable = true)\n",
      " |-- V252: vector (nullable = true)\n",
      " |-- V253: vector (nullable = true)\n",
      " |-- V257: vector (nullable = true)\n",
      " |-- V258: vector (nullable = true)\n",
      " |-- V260: vector (nullable = true)\n",
      " |-- V261: vector (nullable = true)\n",
      " |-- V264: vector (nullable = true)\n",
      " |-- V266: vector (nullable = true)\n",
      " |-- V267: vector (nullable = true)\n",
      " |-- V271: vector (nullable = true)\n",
      " |-- V274: vector (nullable = true)\n",
      " |-- V277: vector (nullable = true)\n",
      " |-- V281: vector (nullable = true)\n",
      " |-- V283: vector (nullable = true)\n",
      " |-- V284: vector (nullable = true)\n",
      " |-- V285: vector (nullable = true)\n",
      " |-- V286: vector (nullable = true)\n",
      " |-- V289: vector (nullable = true)\n",
      " |-- V291: vector (nullable = true)\n",
      " |-- V294: vector (nullable = true)\n",
      " |-- V296: vector (nullable = true)\n",
      " |-- V297: vector (nullable = true)\n",
      " |-- V301: vector (nullable = true)\n",
      " |-- V303: vector (nullable = true)\n",
      " |-- V305: vector (nullable = true)\n",
      " |-- V307: vector (nullable = true)\n",
      " |-- V309: vector (nullable = true)\n",
      " |-- V310: vector (nullable = true)\n",
      " |-- V314: vector (nullable = true)\n",
      " |-- V320: vector (nullable = true)\n",
      " |-- V325: vector (nullable = true)\n",
      " |-- V332: vector (nullable = true)\n",
      " |-- V335: vector (nullable = true)\n",
      " |-- V338: vector (nullable = true)\n",
      " |-- id_01: vector (nullable = true)\n",
      " |-- id_02: vector (nullable = true)\n",
      " |-- id_03: vector (nullable = true)\n",
      " |-- id_04: vector (nullable = true)\n",
      " |-- id_05: vector (nullable = true)\n",
      " |-- id_06: vector (nullable = true)\n",
      " |-- id_07: vector (nullable = true)\n",
      " |-- id_08: vector (nullable = true)\n",
      " |-- id_09: vector (nullable = true)\n",
      " |-- id_10: vector (nullable = true)\n",
      " |-- id_11: vector (nullable = true)\n",
      " |-- ProductCD: vector (nullable = true)\n",
      " |-- card1: vector (nullable = true)\n",
      " |-- card2: vector (nullable = true)\n",
      " |-- card3: vector (nullable = true)\n",
      " |-- card4: vector (nullable = true)\n",
      " |-- card5: vector (nullable = true)\n",
      " |-- card6: vector (nullable = true)\n",
      " |-- addr1: vector (nullable = true)\n",
      " |-- addr2: vector (nullable = true)\n",
      " |-- P_emaildomain: vector (nullable = true)\n",
      " |-- R_emaildomain: vector (nullable = true)\n",
      " |-- M1: vector (nullable = true)\n",
      " |-- M2: vector (nullable = true)\n",
      " |-- M3: vector (nullable = true)\n",
      " |-- M4: vector (nullable = true)\n",
      " |-- M5: vector (nullable = true)\n",
      " |-- M6: vector (nullable = true)\n",
      " |-- M7: vector (nullable = true)\n",
      " |-- M8: vector (nullable = true)\n",
      " |-- M9: vector (nullable = true)\n",
      " |-- id_12: vector (nullable = true)\n",
      " |-- id_13: vector (nullable = true)\n",
      " |-- id_14: vector (nullable = true)\n",
      " |-- id_15: vector (nullable = true)\n",
      " |-- id_16: vector (nullable = true)\n",
      " |-- id_17: vector (nullable = true)\n",
      " |-- id_18: vector (nullable = true)\n",
      " |-- id_19: vector (nullable = true)\n",
      " |-- id_20: vector (nullable = true)\n",
      " |-- id_21: vector (nullable = true)\n",
      " |-- id_22: vector (nullable = true)\n",
      " |-- id_23: vector (nullable = true)\n",
      " |-- id_24: vector (nullable = true)\n",
      " |-- id_25: vector (nullable = true)\n",
      " |-- id_26: vector (nullable = true)\n",
      " |-- id_27: vector (nullable = true)\n",
      " |-- id_28: vector (nullable = true)\n",
      " |-- id_29: vector (nullable = true)\n",
      " |-- id_30: vector (nullable = true)\n",
      " |-- id_31: vector (nullable = true)\n",
      " |-- id_32: vector (nullable = true)\n",
      " |-- id_33: vector (nullable = true)\n",
      " |-- id_34: vector (nullable = true)\n",
      " |-- id_35: vector (nullable = true)\n",
      " |-- id_36: vector (nullable = true)\n",
      " |-- id_37: vector (nullable = true)\n",
      " |-- id_38: vector (nullable = true)\n",
      " |-- DeviceType: vector (nullable = true)\n",
      " |-- DeviceInfo: vector (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Define a list of column names that need to be converted\n",
    "convert_cols = [col_name1 for col_name1, data_type in X_train.dtypes if data_type in [\"integer\", \"double\"]]\n",
    "print(convert_cols)\n",
    "# Loop through the list of columns to apply the VectorAssembler\n",
    "for col_name2 in convert_cols:\n",
    "    assembler = VectorAssembler(inputCols=[col_name2], outputCol=col_name2+\"_vector\",handleInvalid=\"skip\")\n",
    "    X_train = assembler.transform(X_train)\n",
    "    X_test = assembler.transform(X_test)\n",
    "    X_train = X_train.drop(col_name2)\n",
    "    X_test = X_test.drop(col_name2)\n",
    "\n",
    "all_cols = X_train.columns\n",
    "vector_cols = [c for c in all_cols if \"_vector\" in c]\n",
    "for c in vector_cols:\n",
    "    new_name = c.replace(\"_vector\",\"\")\n",
    "    X_train = X_train.withColumnRenamed(c,new_name)\n",
    "    X_test = X_test.withColumnRenamed(c,new_name)\n",
    "# Print the schema of the converted DataFrame\n",
    "# print(X_train.printSchema())\n",
    "print(X_test.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o13259.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 1265) (192.168.0.155 executor driver): java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.stat.SummarizerBuffer.max(Summarizer.scala:710)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:361)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:354)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:573)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:256)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\r\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\r\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:120)\r\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:92)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.stat.SummarizerBuffer.max(Summarizer.scala:710)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:361)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:354)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:573)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:256)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# create scaler and fit to data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler(inputCol\u001b[39m=\u001b[39mcol, outputCol\u001b[39m=\u001b[39mcol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_scaled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m scaler_model \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit(dff)\n\u001b[0;32m     13\u001b[0m \u001b[39m# transform train and test data using fitted scaler\u001b[39;00m\n\u001b[0;32m     14\u001b[0m train \u001b[39m=\u001b[39m scaler_model\u001b[39m.\u001b[39mtransform(X_train)\n",
      "File \u001b[1;32mC:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\ml\\base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32mC:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[1;32m--> 335\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    336\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 332\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mC:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o13259.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 1265) (192.168.0.155 executor driver): java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.stat.SummarizerBuffer.max(Summarizer.scala:710)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:361)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:354)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:573)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:256)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\r\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\r\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:120)\r\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:92)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.stat.SummarizerBuffer.max(Summarizer.scala:710)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:361)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:354)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:573)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:256)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "for col in cols:\n",
    "    if col not in cat_cols:\n",
    "        # concatenate train and test data\n",
    "        dff = X_train.select(col).union(X_test.select(col))\n",
    "\n",
    "        # create scaler and fit to data\n",
    "        scaler = MinMaxScaler(inputCol=col, outputCol=col+\"_scaled\")\n",
    "        scaler_model = scaler.fit(dff)\n",
    "\n",
    "        # transform train and test data using fitted scaler\n",
    "        train = scaler_model.transform(X_train)\n",
    "        test = scaler_model.transform(X_test)\n",
    "\n",
    "        # rename the newly created scaled columns\n",
    "        train = X_train.withColumnRenamed(col+\"_scaled\", col)\n",
    "        test = X_test.withColumnRenamed(col+\"_scaled\", col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
