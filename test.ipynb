{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,isnan,when,sum,lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_object = init_spark()\n",
    "train_identity = spark_object.read.option(\"delimiter\", \",\").csv(\"./data/train_identity.csv\", header='true', inferSchema='true')\n",
    "train_transaction = spark_object.read.option(\"delimiter\", \",\").csv(\"./data/train_transaction.csv\", header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144233\n",
      "590540\n",
      "Row(TransactionID=2987004, id_01=0.0, id_02=70787.0, id_03=None, id_04=None, id_05=None, id_06=None, id_07=None, id_08=None, id_09=None, id_10=None, id_11=100.0, id_12='NotFound', id_13=None, id_14=-480.0, id_15='New', id_16='NotFound', id_17=166.0, id_18=None, id_19=542.0, id_20=144.0, id_21=None, id_22=None, id_23=None, id_24=None, id_25=None, id_26=None, id_27=None, id_28='New', id_29='NotFound', id_30='Android 7.0', id_31='samsung browser 6.2', id_32=32.0, id_33='2220x1080', id_34='match_status:2', id_35='T', id_36='F', id_37='T', id_38='T', DeviceType='mobile', DeviceInfo='SAMSUNG SM-G892A Build/NRD90M')\n",
      "Row(TransactionID=2987000, isFraud=0, TransactionDT=86400, TransactionAmt=68.5, ProductCD='W', card1=13926, card2=None, card3=150.0, card4='discover', card5=142.0, card6='credit', addr1=315.0, addr2=87.0, dist1=19.0, dist2=None, P_emaildomain=None, R_emaildomain=None, C1=1.0, C2=1.0, C3=0.0, C4=0.0, C5=0.0, C6=1.0, C7=0.0, C8=0.0, C9=1.0, C10=0.0, C11=2.0, C12=0.0, C13=1.0, C14=1.0, D1=14.0, D2=None, D3=13.0, D4=None, D5=None, D6=None, D7=None, D8=None, D9=None, D10=13.0, D11=13.0, D12=None, D13=None, D14=None, D15=0.0, M1='T', M2='T', M3='T', M4='M2', M5='F', M6='T', M7=None, M8=None, M9=None, V1=1.0, V2=1.0, V3=1.0, V4=1.0, V5=1.0, V6=1.0, V7=1.0, V8=1.0, V9=1.0, V10=0.0, V11=0.0, V12=1.0, V13=1.0, V14=1.0, V15=0.0, V16=0.0, V17=0.0, V18=0.0, V19=1.0, V20=1.0, V21=0.0, V22=0.0, V23=1.0, V24=1.0, V25=1.0, V26=1.0, V27=0.0, V28=0.0, V29=0.0, V30=0.0, V31=0.0, V32=0.0, V33=0.0, V34=0.0, V35=None, V36=None, V37=None, V38=None, V39=None, V40=None, V41=None, V42=None, V43=None, V44=None, V45=None, V46=None, V47=None, V48=None, V49=None, V50=None, V51=None, V52=None, V53=1.0, V54=1.0, V55=1.0, V56=1.0, V57=0.0, V58=0.0, V59=0.0, V60=0.0, V61=1.0, V62=1.0, V63=0.0, V64=0.0, V65=1.0, V66=1.0, V67=1.0, V68=0.0, V69=0.0, V70=0.0, V71=0.0, V72=0.0, V73=0.0, V74=0.0, V75=1.0, V76=1.0, V77=1.0, V78=1.0, V79=0.0, V80=0.0, V81=0.0, V82=0.0, V83=0.0, V84=0.0, V85=0.0, V86=1.0, V87=1.0, V88=1.0, V89=0.0, V90=0.0, V91=0.0, V92=0.0, V93=0.0, V94=0.0, V95=0.0, V96=1.0, V97=0.0, V98=0.0, V99=0.0, V100=0.0, V101=0.0, V102=1.0, V103=0.0, V104=0.0, V105=0.0, V106=0.0, V107=1.0, V108=1.0, V109=1.0, V110=1.0, V111=1.0, V112=1.0, V113=1.0, V114=1.0, V115=1.0, V116=1.0, V117=1.0, V118=1.0, V119=1.0, V120=1.0, V121=1.0, V122=1.0, V123=1.0, V124=1.0, V125=1.0, V126=0.0, V127=117.0, V128=0.0, V129=0.0, V130=0.0, V131=0.0, V132=0.0, V133=117.0, V134=0.0, V135=0.0, V136=0.0, V137=0.0, V138=None, V139=None, V140=None, V141=None, V142=None, V143=None, V144=None, V145=None, V146=None, V147=None, V148=None, V149=None, V150=None, V151=None, V152=None, V153=None, V154=None, V155=None, V156=None, V157=None, V158=None, V159=None, V160=None, V161=None, V162=None, V163=None, V164=None, V165=None, V166=None, V167=None, V168=None, V169=None, V170=None, V171=None, V172=None, V173=None, V174=None, V175=None, V176=None, V177=None, V178=None, V179=None, V180=None, V181=None, V182=None, V183=None, V184=None, V185=None, V186=None, V187=None, V188=None, V189=None, V190=None, V191=None, V192=None, V193=None, V194=None, V195=None, V196=None, V197=None, V198=None, V199=None, V200=None, V201=None, V202=None, V203=None, V204=None, V205=None, V206=None, V207=None, V208=None, V209=None, V210=None, V211=None, V212=None, V213=None, V214=None, V215=None, V216=None, V217=None, V218=None, V219=None, V220=None, V221=None, V222=None, V223=None, V224=None, V225=None, V226=None, V227=None, V228=None, V229=None, V230=None, V231=None, V232=None, V233=None, V234=None, V235=None, V236=None, V237=None, V238=None, V239=None, V240=None, V241=None, V242=None, V243=None, V244=None, V245=None, V246=None, V247=None, V248=None, V249=None, V250=None, V251=None, V252=None, V253=None, V254=None, V255=None, V256=None, V257=None, V258=None, V259=None, V260=None, V261=None, V262=None, V263=None, V264=None, V265=None, V266=None, V267=None, V268=None, V269=None, V270=None, V271=None, V272=None, V273=None, V274=None, V275=None, V276=None, V277=None, V278=None, V279=0.0, V280=0.0, V281=0.0, V282=1.0, V283=1.0, V284=0.0, V285=0.0, V286=0.0, V287=0.0, V288=0.0, V289=0.0, V290=1.0, V291=1.0, V292=1.0, V293=0.0, V294=1.0, V295=0.0, V296=0.0, V297=0.0, V298=0.0, V299=0.0, V300=0.0, V301=0.0, V302=0.0, V303=0.0, V304=0.0, V305=1.0, V306=0.0, V307=117.0, V308=0.0, V309=0.0, V310=0.0, V311=0.0, V312=0.0, V313=0.0, V314=0.0, V315=0.0, V316=0.0, V317=117.0, V318=0.0, V319=0.0, V320=0.0, V321=0.0, V322=None, V323=None, V324=None, V325=None, V326=None, V327=None, V328=None, V329=None, V330=None, V331=None, V332=None, V333=None, V334=None, V335=None, V336=None, V337=None, V338=None, V339=None)\n"
     ]
    }
   ],
   "source": [
    "print(train_identity.count())\n",
    "print(train_transaction.count())\n",
    "print(train_identity.head())\n",
    "print(train_transaction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_transaction.join(train_identity, on=\"TransactionID\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_identity = spark_object.read.option(\"delimiter\", \",\").csv(\"./data/test_identity.csv\", header='true', inferSchema='true')\n",
    "test_transaction = spark_object.read.option(\"delimiter\", \",\").csv(\"./data/test_transaction.csv\", header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train.count())\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_count = train.count()\n",
    "# def null_percentage(df, column):\n",
    "#     null_count = df.where(col(column).isNull()).count()\n",
    "#     return null_count / total_count\n",
    "\n",
    " \n",
    "# for column in train.columns:\n",
    "#     if null_percentage(train, column) > 0.05:\n",
    "#         train = train.drop(col(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some ids are incorrectly labelled, we will correct it first\n",
    "id_cols = [col for col in test_identity.columns if col.startswith('id')]\n",
    "rename_cols = {col: 'id_' + col[-2:] for col in id_cols}\n",
    "test_identity = test_identity.select([col(c).alias(rename_cols.get(c, c)) for c in test_identity.columns])\n",
    "# join test_transaction and test_identity on TransactionID\n",
    "test = test_transaction.join(test_identity,on=\"TransactionID\",how='left')\n",
    "train.write.csv('./data/train_combined.csv',header = True)\n",
    "test.write.csv('./data/test_combined.csv',header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 434)\n",
      "(506691, 433)\n"
     ]
    }
   ],
   "source": [
    "train_combined = spark_object.read.option(\"delimiter\", \",\").csv(\"data/train_combined.csv\", header='true', inferSchema='true')\n",
    "test_combined = spark_object.read.option(\"delimiter\", \",\").csv(\"data/test_combined.csv\", header='true', inferSchema='true')\n",
    "\n",
    "print((train_combined.count(),len(train_combined.columns)))\n",
    "print((test_combined.count(),len(test_combined.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_missing_cols(df, n=None, thresh=80):\n",
    "    \"\"\"\n",
    "    returns missing columns in dataframe with missing values percent > thresh\n",
    "    if n=None. It will gave whole dataframe with missing values percent > thresh\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the percentage of missing values for each column\n",
    "    dff = df.select([(count(when(isnan(c) | col(c).isNull(), c))/count('*'))*100 for c in df.columns])\n",
    "    dff = dff.toDF(*df.columns)\n",
    "\n",
    "    # Count the total number of columns in the DataFrame\n",
    "    total_cols = len(df.columns)\n",
    "\n",
    "    # Convert the DataFrame to Pandas and process it\n",
    "    dff = dff.toPandas()\n",
    "    dff = dff.melt(var_name='col', value_name='missing_percent')\n",
    "    dff = dff.sort_values(by=['missing_percent'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(f'There are {total_cols} columns in this dataset.')\n",
    "    print(f'There are {dff[dff[\"missing_percent\"] > thresh].shape[0]} columns with missing percent values than {thresh}%')\n",
    "    \n",
    "    # Return the top missing columns\n",
    "    if n:\n",
    "        return dff.head(n)\n",
    "    else:\n",
    "        return dff\n",
    "\n",
    "# Get the columns with missing percentage greater than 50%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Program Files\\Apache Spark\\spark-3.1.2-bin-hadoop2.7\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 434 columns in this dataset.\n",
      "There are 214 columns with missing percent values than 50%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_missing = top_missing_cols(train_combined, n=None, thresh=50)\n",
    "missing_cols = df_missing['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nan_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m col_name \u001b[39min\u001b[39;00m missing_cols:\n\u001b[1;32m----> 3\u001b[0m     count \u001b[39m=\u001b[39m train_combined\u001b[39m.\u001b[39mselect([count(when(col(c)\u001b[39m.\u001b[39misNull(), c))\u001b[39m.\u001b[39malias(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m [col_name]])\u001b[39m.\u001b[39mcollect()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m         nan_dict[count]\u001b[39m.\u001b[39mappend(col_name)\n",
      "Cell \u001b[1;32mIn[158], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m nan_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m col_name \u001b[39min\u001b[39;00m missing_cols:\n\u001b[1;32m----> 3\u001b[0m     count \u001b[39m=\u001b[39m train_combined\u001b[39m.\u001b[39mselect([count(when(col(c)\u001b[39m.\u001b[39;49misNull(), c))\u001b[39m.\u001b[39malias(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m [col_name]])\u001b[39m.\u001b[39mcollect()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m         nan_dict[count]\u001b[39m.\u001b[39mappend(col_name)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "nan_dict = {}\n",
    "for col_name in missing_cols:\n",
    "    count = train_combined.select([count(when(col(c).isNull(), c)).alias(c) for c in [col_name]]).collect()[0][0]\n",
    "    try:\n",
    "        nan_dict[count].append(col_name)\n",
    "    except:\n",
    "        nan_dict[count] = [col_name]\n",
    "\n",
    "for k,v in nan_dict.items():\n",
    "    print(f'#####' * 4)\n",
    "    print(f'NAN count = {k} percent: {(int(k)/train.count())*100} %')\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
